Deploying Application Stacks
Docker Swarm uses the standard Compose specification, with some additional sections to configure Swarm features. 
Some parts of the spec only apply when you run the app with Compose and others only apply when you run in Swarm - both runtimes ignore sections that aren't relevant.

You deploy applications as stacks in Swarm mode. A stack is defined in a single Compose file, and it can contain definitions for services and for other Docker resources - like config objects, secrets and networks

While running on PROD, there are a few Infra level services that can be shared across different product/services.

Shared components - infrastructure
In a production cluster you can have some components which are shared between multiple apps. The message queue and reverse proxy from the sample application are good candidates - they're defined in the Compose file infrastructure.yml.

infrastructure.yml
=================
version: "3.8"

services:

  proxy:
    image: traefik:v2.3
    command:
      - "--providers.docker"
      - "--providers.docker.swarmMode=true"
      - "--providers.docker.endpoint=unix:///var/run/docker.sock"
      - "--providers.docker.exposedbydefault=false"
      - "--providers.docker.network=frontend-net"
      - "--entrypoints.web.address=:80"
      - "--api.insecure=true"
    ports:
      - "8080:80"
      - "8088:8080"
    volumes:
      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock
    networks:
      - frontend-net
    deploy:
      placement:
        constraints:
          - "node.platform.os == linux"
          - "node.role == manager"

  message-queue:
    image: nats:2.1
    networks:
      - backend-net
    deploy:
      placement:
        constraints:
          - "node.platform.os == linux"

networks:
  frontend-net:
    external: true
    name: frontend-net

  backend-net:
    external: true
    name: backend-net
=================

Reverse Proxy - traefik
Binding to Docker Socket. Only look for containers attached to front-end network.
Constraint - Run on Linux platform on a node that has role of Manager.

message-queue - NATS
    Constratins -> For message queue deployment, this needs to run on NODE where OS = LINUX

This is a standard Compose file, with the additional deploy section which only applies in Swarm mode. It tells the Swarm that these services should run on Linux nodes in the cluster.

Create the networks and deploy the stack:
==============
vagrant ssh manager 

docker network create -d overlay frontend-net

docker network create -d overlay backend-net

cd /docker4dotnet

docker stack deploy -c app/05/05-03/infrastructure.yml infrastructure
==============
Explanation:
Creating separate networks - frontend-net and backend-net
Creating "overlay" type of network here - Networking driver in docker swarm mode that means all containers can connect to each other irrespective of whichever server they're on provided they're on the same network.
Here, we're deploying infrastructure.yml as a stack and giving it a name of infrastructure. This is similar to docker compose deployments

Once deployment is done, we can check the stack services using this commands:
===========
docker stack ps infrastructure
docker service logs infrastructure_proxy
===========
We can see the servcies running on which Node, what's their state, current state, image its using etc.

STORING APP CONFIGURATION
===============
Rather than doing volume mounts, when running in swarm mode, we can use this "config" resource to create config objects. 
Config objects can be of any foramats - JSON/XML or Binaries as well.
The Swarm cluster has a distributed database running in the manager node(s). It's where all the stack and service specifications are stored so you can manage apps without needing the Compose file. It's also the storage layer for application config.

Config objects are Docker resources in Swarm mode. You create them from files, do we can store the signup-web-appsettings.json as a config object and mount it into the container filesystem. Config objects can be any format - JSON, XML or even binary files.
Create all the config objects for the app components:
========
docker config create signup-web-appsettings app/05/05-03/configs/signup-web-appsettings.json

docker config create signup-web-log4net app/05/05-03/configs/signup-web-log4net.config

docker config create reference-data-config app/05/05-03/configs/reference-data-config.json
========

Check the configs and the contents:
========
docker config ls

docker config inspect --pretty signup-web-log4net

docker config inspect --pretty reference-data-config
========
These are non-sensitive data that can be ok for someone to view.

For storing sensitive information, we can create secrets.

STORING SENSITIVE APP SETTINGS
Anyone with access to the cluster can see the contents of a config object, so you shouldn't use them to store sensitive data like passwords or API keys. Instead you store those settings in secret objects, which are encrypted in the Swarm.

You create secrets from a file just like config objects, and they can be mounted into the container filesystem. They're encrypted at rest and in transit - the plain text is only visible inside the container.
Create secrets for the app's sensitive config settings:
========
docker secret create signup-db-password app/05/05-03/secrets/signup-db-password

docker secret create signup-web-connectionstrings app/05/05-03/secrets/signup-web-connectionStrings.config

docker secret create save-handler-connectionstrings app/05/05-03/secrets/save-handler-connectionStrings.config

docker secret create reference-data-secret app/05/05-03/secrets/reference-data-secret.json
========
Check the secrets and the contents:
========
docker secret ls

docker secret inspect --pretty reference-data-secret
========
You can't read the contents of a secret in the cluster. The only way to read it is from inside a container which mounts the secret.

When we inspect on secret, we will not see the actual data.
The only way to read this secret is from within the container. They're encrypted by default or in-transit.

DEPLOYING MAIN APPLICATION
The core parts of the sample application are modelled in 05-03/signup.yml. There's a mix of Windows and Linux components which will be spread around the nodes.The spec includes labels so the front-end components get found by Trafik, and mounts for the config and secret objects.

All the images are published on Docker Hub. Swarm nodes have to be able to access images so they need to be pushed to a registry. These have all been built from the Dockerfiles in the 05-03-deploying-stacks folder - there's nothing new except in the signup-web/start.ps1 which has some logic to set up the config files.

signup.yml
==========
version: "3.8"

services:
  signup-db:
    image: docker4dotnet/signup-db:05-03
    networks:
      - signup-net
    environment:
      - SA_PASSWORD_PATH=C:\db\config\sa-password.txt
    secrets:
      - source: signup-db-password
        target: C:\db\config\sa-password.txt
    deploy:
      placement:
        constraints:
          - "node.platform.os == windows"

  signup-web:
    image: docker4dotnet/signup-web:05-03
    networks:
      - signup-net
      - frontend-net
      - backend-net
    configs:
      - source: signup-web-appsettings
        target: C:\web-app\configs\config.json
      - source: signup-web-log4net
        target: C:\web-app\config\log4net.config
    secrets:
      - source: signup-web-connectionstrings
        target: C:\web-app\config\connectionStrings.config
    deploy:
      placement:
        constraints:
          - "node.platform.os == windows"
      labels:
        - "traefik.enable=true"
        - "traefik.http.routers.web.rule=PathPrefix(`/app/`)"
        - "traefik.http.services.signup-web.loadbalancer.server.port=80"

  homepage:
    image: docker4dotnet/homepage:05-03
    networks:
      - frontend-net
    deploy:
      placement:
        constraints:
          - "node.platform.os == windows"
      labels:
        - "traefik.enable=true"
        - "traefik.http.routers.homepage.rule=PathPrefix(`/`)"
        - "traefik.http.services.homepage.loadbalancer.server.port=80"

  save-handler:
    image: docker4dotnet/save-handler:05-03    
    networks:
      - signup-net
      - backend-net
    secrets:
      - source: save-handler-connectionstrings
        target: C:\save-prospect-handler\config\connectionStrings.config
    deploy:
      placement:
        constraints:
          - "node.platform.os == windows"

  reference-data-api:
    image: docker4dotnet/reference-data-api:05-03
    networks:
      - signup-net
      - frontend-net
    configs:
      - source: reference-data-config
        target: /app/configs/config.json
    secrets:
      - source: reference-data-secret
        target: /app/secrets/secret.json
    deploy:
      placement:
        constraints:
          - "node.platform.os == linux"
      labels:
        - "traefik.enable=true"
        - "traefik.http.routers.api.rule=PathPrefix(`/api/`)"
        - "traefik.http.services.reference-data-api.loadbalancer.server.port=80"

networks:
  signup-net:
    external: true
    name: signup-net

  frontend-net:
    external: true
    name: frontend-net

  backend-net:
    external: true
    name: backend-net

configs:
  signup-web-appsettings:
    external: true

  signup-web-log4net:
    external: true
  
  reference-data-config:
    external: true

secrets:
  signup-db-password:
    external: true
  
  signup-web-connectionstrings:
    external: true
  
  save-handler-connectionstrings:
    external: true
  
  reference-data-secret:
    external: true
==========
Explanation:
    Addition to existing compose file.
    Added deploy sections to individual containers.
    For DB: 
        Source of secret is the secret we created in the previous section.
        Target = File inside container where we want the secret to be copied.
        Deploy.placement.Constraint - This should go to NODE with Windows as the OS

        This is much secure

    Signup-Web
        Attached to multiple networks.
        Individual config objects source and target mapping.
        Secrets also configured here. 
        Deployment constraint - Node with Windows as Platform OS.

    For all networks, configs & secrets, we've set external as true. Kind of acts as pre-requisite for them to be there before the application components are deployed.

DEPLOY ANALYTICS PART OF APP AS SEPARATE STACK

The core app doesn't include the analytics components - those are modelled separately in 05-03/analytics.yml. The services connect to whichever networks they need, so you can model one large application as subsystems in different stacks.

analytics.yml
===========
version: "3.8"

services:

  elasticsearch:
    image: elasticsearch:6.8.12
    environment:
      - discovery.type=single-node
    networks:
      - analytics-net
    deploy:
      placement:
        constraints:
          - "node.platform.os == linux"

  kibana:
    image: kibana:6.8.12
    ports:
      - "5601:5601"
    networks:
      - analytics-net
    deploy:
      placement:
        constraints:
          - "node.platform.os == linux"

  index-handler:
    image: docker4dotnet/index-handler:05-03
    networks:
      - backend-net
      - analytics-net
    deploy:
      placement:
        constraints:
          - "node.platform.os == linux"

networks:
  analytics-net:
    external: true
    name: analytics-net

  backend-net:
    external: true
    name: backend-net
===========
Explanation:
    Entire analytics stack is deployed on linux OS

Deploy the analytics stack:
=============
docker network create -d overlay analytics-net

docker stack deploy -c app/05/05-03/analytics.yml analytics

docker stack ps analytics
=============
Test the app again with another sign up

Verify the message is processed by both handlers:
=============
docker service logs signup_save-handler

docker service logs analytics_index-handler
=============
Browse to any node on port 5601 and check the data in Kibana.

Modelling production concerns
The app is running now, in a container platform which provides high availability and scale.

We're not really making use of those features with our simple app definitions, so next we'll extend them to make the application more suitable for production.